Plan -- | -- | -- (05/28/2013)

Objectives:
	1. to distill sentiment about user feedback from comments/reviews
	2. use subjectivity analysis to get at common themes and root causes of sentiment
	3. to return results that prioritize actionable areas, i.e. 
		- find problems
		- feature requests
		- good features


Ideas/Approaches:
	- Brown clustering: 
		- after splitting into 
	- neural networks:
		- could use supervised (review,rating) to generate a function
		- unsupervised can be used in clustering
	- naive Bayes classifier:
	- classify the sentences into (pos,neg,neut,both) then work on clustering them into catagories, special work for the both ones


Features:
	- words and pos/neg score from SentiWordNet 3.0
	- common words across a cluster

Plan update -- | -- | --(06/21/2013)
	- Goals: Build a system that facilitates the rappid understanding of user feedback. This invovles understanding the polarity (or ambiguity) of sentiment, then extracting the important features common to a particular sentiment group. This will simplify the process of identifying major product issues.

	- Steps and notes:
		1. Group comments into three catagories (POS/NEG/BTH). Neutral is omitted because it doesn't appear in any of the datasets, as far as I can tell.
		2. The process of grouping will likely require a sent. score of some kind. This will give us a metric of how positive or negative a comment (or part of one) is.
		3. Attempt to determine what features of the product are liked/disliked and group them to determine common issues across the sentiment class.
		- At each step the accuracy must be determined using an annotated testing set.
	 
Action:
	- Attempted to classify reviews by rating using user ratings as training/validation set. This did'nt work very well; ratings often contain multiple sentiments which makes classification based on rating difficult.
	- Used movie reviews to classify into pos/neg. Appeared to work well, but of course many comments contain both positive and negative sentiments. Thus, more classes are necessary.
	- Classified ~260 comments into pos/neg/neut/both and attempted to classify using unigram features. However, this doesn't seem to work very well. I will classify more data and see if that improves performance.


Work -- | -- | --(06/26/2013)

	Two annotator results of 100 reviews

			D a n i e l

		S			pos | neg | neut | both
		a	pos 	45	|  3  |	 3   | 7
		s	-------------------------------
		s	neg 	16	|  1  |  0   | 3
		d	-------------------------------
		a	neut    6	|  3  |  1   | 0
			-------------------------------
			both    4 	|  5  |  1   | 2


	k = [0.49 - (0.412+0.024+0.005+0.0144)]/(1-0.455) = 0.0642 ?????
	Cohen's kappa = 61.73

Work -- | -- | -- (06/27/2013)

	I attempted using 4-grams, but they reduced accuracy from ~76% to ~74%. I also tried using a different tokenizer, but that also reduced accuracy. Putting in the counts for each word had a small negative effect. 

	Bigrams and trigrams w/ pos tagging appear to be worse. Thus, the best accuracy has been with simple uni, bi, tri

	
Work -- | -- | -- (07/02/2013)

	I attempted to sum up the positive and negative scores from SentiWordnet 3.0. The results were very inconclusive as the groups overlapped extensively (pos/neg being the most dichotomized). I am working to get a semantic analysis tool to work to have a more specific score for each word form. Also, I need to figure out a way to deal with negated phrases. 
Work -- | -- | -- (07/10/2013)

	Have incorporated the SentiWordNet 3.0 scores to form ordered pairs. However, I am having a lot of difficulty figuring out how to use the various SVM packages. The most successful thus far has been the pysvmlight. I will probably go back to Naive Bayes for simplicity and more features. At Heemeng's request I have reannotated the data removing the neut/both classes. This will hopefully yeald a higher level of accuracy  than the four classes. I am worried that I may not make sufficient progress (perhaps my goals were to ambitious, but I don't think so).




